# **Python Revision Guide: Web Scraping with BeautifulSoup ğŸ•·ï¸**

Web scraping is used to extract data from websites.

Pythonâ€™s `BeautifulSoup` library makes it easy to parse HTML/XML and extract required content.

âœ… **Install Dependencies:**

```bash
pip install beautifulsoup4 requests lxml
```

---

## **1. Fetching a Web Page**

```python
import requests
from bs4 import BeautifulSoup

# Fetch the webpage
url = "https://example.com"
response = requests.get(url)

# Parse HTML
soup = BeautifulSoup(response.text, "html.parser")

print(soup.prettify())  # Prints formatted HTML
```

ğŸ”¹ **Use `html.parser` or `lxml` (faster).**

ğŸ”¹ **`prettify()` helps visualize the structure.**

---

## **2. Extracting Specific Elements**

### **ğŸ‘‰ Get the Page Title**

```python
title = soup.title.text
print(title)
```

### **ğŸ‘‰ Get All Headings (`h1`, `h2`, etc.)**

```python
headings = [h.text for h in soup.find_all("h1")]
print(headings)
```

### **ğŸ‘‰ Get a Specific Tag by Class**

```python
div_content = soup.find("div", class_="content").text
print(div_content)
```

### **ğŸ‘‰ Get a Specific Tag by ID**

```python
main_section = soup.find("section", id="main").text
print(main_section)
```

---

## **3. Extracting Links & Images**

### **ğŸ‘‰ Get All Links (`<a>` tags)**

```python
links = [a["href"] for a in soup.find_all("a", href=True)]
print(links)
```

### **ğŸ‘‰ Get All Images (`<img>` tags)**

```python
images = [img["src"] for img in soup.find_all("img", src=True)]
print(images)
```

---

## **4. Navigating the HTML Tree**

### **ğŸ‘‰ Parent & Children Elements**

```python
parent = soup.find("div", class_="content").parent
children = list(soup.find("div", class_="content").children)

print(parent)
print(children)
```

### **ğŸ‘‰ Next & Previous Sibling**

```python
next_element = soup.find("h1").find_next_sibling()
previous_element = soup.find("h1").find_previous_sibling()

print(next_element)
print(previous_element)
```

---

## **5. Extracting Data from Tables**

```python
table = soup.find("table")
rows = table.find_all("tr")

for row in rows:
    cols = row.find_all("td")
    print([col.text for col in cols])
```

ğŸ”¹ **Useful for stock prices, product listings, etc.**

---

## **6. Handling JavaScript-Rendered Content**

If data is loaded dynamically (e.g., using JavaScript), use **Selenium** or **API calls** instead.

âœ… **Try Checking API Calls First:**

Use **Browser Developer Tools â†’ Network Tab** to see if an API returns the data.

âœ… **Use Selenium for JavaScript-based Sites:**

```python
from selenium import webdriver
from bs4 import BeautifulSoup

driver = webdriver.Chrome()
driver.get("https://example.com")

soup = BeautifulSoup(driver.page_source, "html.parser")
print(soup.prettify())

driver.quit()
```

ğŸ”¹ **Selenium automates browser interactions when `requests` isnâ€™t enough.**

---

## **7. Storing Scraped Data**

### **ğŸ‘‰ Save to CSV**

```python
import csv

data = [["Name", "Price"], ["Laptop", "$1000"], ["Phone", "$500"]]

with open("products.csv", "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerows(data)
```

### **ğŸ‘‰ Save to JSON**

```python
import json

data = {"name": "Laptop", "price": "$1000"}

with open("data.json", "w") as f:
    json.dump(data, f)
```

---

## **8. Handling Anti-Scraping Measures**

### **ğŸ‘‰ Use Headers to Mimic a Browser**

```python
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}
response = requests.get("https://example.com", headers=headers)
```

### **ğŸ‘‰ Use Randomized Delays**

```python
import time
import random

time.sleep(random.uniform(2, 5))
```

### **ğŸ‘‰ Rotate Proxies for Large-Scale Scraping**

Use `scrapy` or `proxy services` to prevent blocks.

---

## **ğŸ“Œ Summary**

âœ… **Use `requests` to fetch pages & `BeautifulSoup` to parse HTML.**

âœ… **Extract text, links, images, and tables easily.**

âœ… **Use Selenium for JavaScript-rendered pages.**

âœ… **Store scraped data in CSV, JSON, or a database.**

âœ… **Respect `robots.txt` and avoid getting blocked!**
